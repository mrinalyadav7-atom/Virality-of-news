{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from newspaper import Article  \n",
    "import csv \n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://timesofindia.indiatimes.com/world\"\n",
    "r = requests.get(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(r.content, 'html5lib') \n",
    "table = soup.findAll('a', attrs = {'class':'w_img'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news=[]\n",
    "for row in table: \n",
    "    if not row['href'].startswith('http'):\n",
    "        news.append('https://timesofindia.indiatimes.com'+row['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=[]\n",
    "for i in news:\n",
    "    article = Article(i, language=\"en\")\n",
    "    article.download() \n",
    "    article.parse() \n",
    "    article.nlp() \n",
    "    data={}\n",
    "    data['Title']=article.title\n",
    "    data['Text']=article.text\n",
    "    data['Summary']=article.summary\n",
    "    data['Keywords']=article.keywords\n",
    "    df.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=pd.DataFrame(df)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILEPATH=\"E:\\Machine_Learning_Models\\OnlineNewsPopularity\\OnlineNewsPopularity.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_cols(data):\n",
    "    \"\"\"Clean the column names by stripping and lowercase.\"\"\"\n",
    "    clean_col_map = {x: x.lower().strip() for x in list(data)}\n",
    "    return data.rename(index=str, columns=clean_col_map)\n",
    "\n",
    "def TrainTestSplit(X, Y, R=0, test_size=0.2):\n",
    "    \"\"\"Easy Train Test Split call.\"\"\"\n",
    "    return train_test_split(X, Y, test_size=test_size, random_state=R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = clean_cols(pd.read_csv(FILEPATH))\n",
    "train_set, test_set = train_test_split(full_data, test_size=0.20, random_state=42)\n",
    "\n",
    "x_train = train_set.drop(['url','shares', 'timedelta', 'lda_00','lda_01','lda_02','lda_03','lda_04','num_self_hrefs', 'kw_min_min', 'kw_max_min', 'kw_avg_min','kw_min_max','kw_max_max','kw_avg_max','kw_min_avg','kw_max_avg','kw_avg_avg','self_reference_min_shares','self_reference_max_shares','self_reference_avg_sharess','rate_positive_words','rate_negative_words','abs_title_subjectivity','abs_title_sentiment_polarity'], axis=1)\n",
    "y_train = train_set['shares']\n",
    "\n",
    "x_test = test_set.drop(['url','shares', 'timedelta', 'num_self_hrefs', 'kw_min_min', 'kw_max_min', 'kw_avg_min','kw_min_max','kw_max_max','kw_avg_max','kw_min_avg','kw_max_avg','kw_avg_avg','self_reference_min_shares','self_reference_max_shares','self_reference_avg_sharess','rate_positive_words','rate_negative_words','abs_title_subjectivity','abs_title_sentiment_polarity'], axis=1)\n",
    "y_test = test_set['shares']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestRegressor(random_state=42)\n",
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_res = pd.DataFrame(clf.predict(x_train),list(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_res.reset_index(level=0, inplace=True)\n",
    "rf_res_df = rf_res.rename(index=str, columns={\"index\": \"Actual shares\", 0: \"Predicted shares\"})\n",
    "rf_res_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.corpus import stopwords\n",
    "stopwords=set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate_unique(words):\n",
    "    words=tokenize(words)\n",
    "    no_order = list(set(words))\n",
    "    rate_unique=len(no_order)/len(words)\n",
    "    return rate_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate_nonstop(words):\n",
    "    words=tokenize(words)\n",
    "    filtered_sentence = [w for w in words if not w in stopwords]\n",
    "    rate_nonstop=len(filtered_sentence)/len(words)\n",
    "    no_order = list(set(filtered_sentence))\n",
    "    rate_unique_nonstop=len(no_order)/len(words)\n",
    "    return rate_nonstop,rate_unique_nonstop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_token(words):\n",
    "    words=tokenize(words)\n",
    "    length=[]\n",
    "    for i in words:\n",
    "        length.append(len(i))\n",
    "    return np.average(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datefinder\n",
    "import datetime  \n",
    "from datetime import date \n",
    "def day(article_text):\n",
    "    article=article_text\n",
    "    if len(list(datefinder.find_dates(article)))>0:\n",
    "        date=str(list(datefinder.find_dates(article))[0])\n",
    "        date=date.split()\n",
    "        date=date[0]\n",
    "        year, month, day = date.split('-')     \n",
    "        day_name = datetime.date(int(year), int(month), int(day)) \n",
    "        return day_name.strftime(\"%A\")\n",
    "    return \"Monday\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    text=text\n",
    "    return word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_words=[]\n",
    "neg_words=[]\n",
    "def polar(words):\n",
    "    all_tokens=tokenize(words)\n",
    "    for i in all_tokens:\n",
    "        analysis=TextBlob(i)\n",
    "        polarity=analysis.sentiment.polarity\n",
    "        if polarity>0:\n",
    "            pos_words.append(i)\n",
    "        if polarity<0:\n",
    "            neg_words.append(i)\n",
    "    return pos_words,neg_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rates(words):\n",
    "    words=polar(words)\n",
    "    pos=words[0]\n",
    "    neg=words[1]\n",
    "    all_words=words\n",
    "    global_rate_positive_words=(len(pos)/len(all_words))/100\n",
    "    global_rate_negative_words=(len(neg)/len(all_words))/100\n",
    "    pol_pos=[]\n",
    "    pol_neg=[]\n",
    "    for i in pos:\n",
    "        analysis=TextBlob(i)\n",
    "        pol_pos.append(analysis.sentiment.polarity)\n",
    "        avg_positive_polarity=analysis.sentiment.polarity\n",
    "    for j in neg:\n",
    "        analysis2=TextBlob(j)\n",
    "        pol_neg.append(analysis2.sentiment.polarity)\n",
    "        avg_negative_polarity=analysis2.sentiment.polarity\n",
    "    min_positive_polarity=min(pol_pos)\n",
    "    max_positive_polarity=max(pol_pos)\n",
    "    min_negative_polarity=min(pol_neg)\n",
    "    max_negative_polarity=max(pol_neg)\n",
    "    avg_positive_polarity=np.average(pol_pos)\n",
    "    avg_negative_polarity=np.average(pol_neg)\n",
    "    return global_rate_positive_words,global_rate_negative_words,avg_positive_polarity,min_positive_polarity,max_positive_polarity,avg_negative_polarity,min_negative_polarity,max_negative_polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=[]\n",
    "for i in news:\n",
    "    pred_info={}\n",
    "    article = Article(i, language=\"en\") # en for English \n",
    "    article.download() \n",
    "    article.parse()\n",
    "    analysis=TextBlob(article.text)\n",
    "    polarity=analysis.sentiment.polarity\n",
    "    title_analysis=TextBlob(article.title)\n",
    "    pred_info['text']=article.text\n",
    "    pred_info['n_tokens_title']=len(tokenize(article.title))\n",
    "    pred_info['n_tokens_content']=len(tokenize(article.text))\n",
    "    pred_info['n_unique_tokens']=rate_unique(article.text)\n",
    "    pred_info['n_non_stop_words']=rate_nonstop(article.text)[0]\n",
    "    pred_info['n_non_stop_unique_tokens']=rate_nonstop(article.text)[1]\n",
    "    pred_info['num_hrefs']=article.html.count(\"https://timesofindia.indiatimes.com\")\n",
    "    pred_info['num_imgs']=len(article.images)\n",
    "    pred_info['num_videos']=len(article.movies)\n",
    "    pred_info['average_token_length']=avg_token(article.text)\n",
    "    pred_info['num_keywords']=len(article.keywords)\n",
    "    \n",
    "    if \"life-style\" in article.url:\n",
    "        pred_info['data_channel_is_lifestyle']=1\n",
    "    else:\n",
    "        pred_info['data_channel_is_lifestyle']=0\n",
    "    if \"etimes\" in article.url:\n",
    "        pred_info['data_channel_is_entertainment']=1\n",
    "    else:\n",
    "        pred_info['data_channel_is_entertainment']=0\n",
    "    if \"business\" in article.url:\n",
    "        pred_info['data_channel_is_bus']=1\n",
    "    else:\n",
    "        pred_info['data_channel_is_bus']=0\n",
    "        if \"social media\" or \"facebook\" or \"whatsapp\" in article.text.lower():\n",
    "        data_channel_is_socmed=1\n",
    "        data_channel_is_tech=0\n",
    "        data_channel_is_world=0\n",
    "    else:\n",
    "        data_channel_is_socmed=0\n",
    "    if (\"technology\" or \"tech\" in article.text.lower()) or (\"technology\" or \"tech\" in article.url):\n",
    "        data_channel_is_tech=1\n",
    "        data_channel_is_socmed=0\n",
    "        data_channel_is_world=0\n",
    "    else:\n",
    "        data_channel_is_tech=0\n",
    "    if \"world\" in article.url:\n",
    "        data_channel_is_world=1\n",
    "        data_channel_is_tech=0\n",
    "        data_channel_is_socmed=0\n",
    "    else:\n",
    "        data_channel_is_world=0\n",
    "        \n",
    "    pred_info['data_channel_is_socmed']=data_channel_is_socmed\n",
    "    pred_info['data_channel_is_tech']=data_channel_is_tech\n",
    "    pred_info['data_channel_is_world']=data_channel_is_world\n",
    "    \n",
    "    if day(i)==\"Monday\":\n",
    "        pred_info['weekday_is_monday']=1\n",
    "    else:\n",
    "        pred_info['weekday_is_monday']=0\n",
    "    if day(i)==\"Tuesday\":\n",
    "        pred_info['weekday_is_tuesday']=1\n",
    "    else:\n",
    "        pred_info['weekday_is_tuesday']=0\n",
    "    if day(i)==\"Wednesday\":\n",
    "        pred_info['weekday_is_wednesday']=1\n",
    "        else:\n",
    "        pred_info['weekday_is_wednesday']=0\n",
    "    if day(i)==\"Thursday\":\n",
    "        pred_info['weekday_is_thursday']=1\n",
    "    else:\n",
    "        pred_info['weekday_is_thursday']=0\n",
    "    if day(i)==\"Friday\":\n",
    "        pred_info['weekday_is_friday']=1\n",
    "    else:\n",
    "        pred_info['weekday_is_friday']=0\n",
    "    if day(i)==\"Saturday\":\n",
    "        pred_info['weekday_is_saturday']=1\n",
    "        pred_info['is_weekend']=1\n",
    "    else:\n",
    "        pred_info['weekday_is_saturday']=0\n",
    "    if day(i)==\"Sunday\":\n",
    "        pred_info['weekday_is_sunday']=1\n",
    "        pred_info['is_weekend']=1\n",
    "    else:\n",
    "        pred_info['weekday_is_sunday']=0\n",
    "        pred_info['is_weekend']=0\n",
    "        \n",
    "    pred_info['global_subjectivity']=analysis.sentiment.subjectivity\n",
    "    pred_info['global_sentiment_polarity']=analysis.sentiment.polarity\n",
    "    pred_info['global_rate_positive_words']=rates(article.text)[0]\n",
    "    pred_info['global_rate_negative_words']=rates(article.text)[1]\n",
    "    pred_info['avg_positive_polarity']=rates(article.text)[2]\n",
    "    pred_info['min_positive_polarity']=rates(article.text)[3]\n",
    "    pred_info['max_positive_polarity']=rates(article.text)[4]\n",
    "    pred_info['avg_negative_polarity']=rates(article.text)[5]\n",
    "    pred_info['min_negative_polarity']=rates(article.text)[6]\n",
    "    pred_info['max_negative_polarity']=rates(article.text)[7]    \n",
    "     pred_info['title_subjectivity']=title_analysis.sentiment.subjectivity\n",
    "    pred_info['title_sentiment_polarity']=title_analysis.sentiment.polarity\n",
    "    df2.append(pred_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df=pd.DataFrame(df2)\n",
    "pred_test=pred_df.drop(['text'],axis=1)\n",
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2=pd.DataFrame(clf.predict(pred_test),pred_df['text'])\n",
    "test2.reset_index(level=0, inplace=True)\n",
    "test2 = test2.rename(index=str, columns={\"index\": \"News\", 0: \"Virality\"})\n",
    "test2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
